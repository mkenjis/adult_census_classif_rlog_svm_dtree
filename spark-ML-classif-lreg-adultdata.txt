import org.apache.spark._
import org.apache.spark.sql._
import spark.sqlContext.implicits._
import org.apache.spark.sql.types._

val census_data = sc.textFile("adultdata/adult.data").map(row => row.split(","))

val adultschema = StructType(Array(
 StructField("age",LongType,true),
 StructField("workclass",StringType,true),
 StructField("fnlwgt",LongType,true),
 StructField("education",StringType,true),
 StructField("marital_status",StringType,true),
 StructField("occupation",StringType,true),
 StructField("relationship",StringType,true),
 StructField("race",StringType,true),
 StructField("sex",StringType,true),
 StructField("capital_gain",LongType,true),
 StructField("capital_loss",LongType,true),
 StructField("hours_per_week",LongType,true),
 StructField("native_country",StringType,true),
 StructField("income",StringType,true)
))

// x(4) : remove education-num
val dfraw = census_data.map(cols => Row(cols(0).trim.toLong,cols(1),cols(2).trim.toLong,cols(3),cols(5),cols(6),cols(7),cols(8),
                                        cols(9),cols(10).trim.toLong,cols(11).trim.toLong,cols(12).trim.toLong,cols(13),cols(14)))
										  
val df = spark.sqlContext.createDataFrame(dfraw,adultschema)
										  
---------------------------------------

val census_data = sc.textFile("adultdata/adult.data").map(row => row.split(","))

case class Adult(
  age : Long,
  workclass : String,
  fnlwgt : Long,
  education : String,
  marital_status : String,
  occupation : String,
  relationship : String,
  race : String,
  sex : String,
  capital_gain : Long,
  capital_loss : Long,
  hours_per_week : Long,
  native_country : String,
  income : String
)

// x(4) : remove education-num
val dfrdd = census_data.map(cols => Adult(cols(0).trim.toLong,cols(1),cols(2).trim.toLong,cols(3),cols(5),cols(6),cols(7),cols(8),
                                          cols(9),cols(10).trim.toLong,cols(11).trim.toLong,cols(12).trim.toLong,cols(13),cols(14)))

val dfraw = dfrdd.toDF()

----------------------------------

import org.apache.spark.sql.DataFrame
import org.apache.spark.ml.feature._

val dfrawrp = dfraw.na.replace(Array("workclass"), Map("?" -> "Private"))
val dfrawrpl = dfrawrp.na.replace(Array("occupation"),  Map("?" -> "Prof-specialty"))
val dfrawnona = dfrawrpl.na.replace(Array("native_country"), Map("?" -> "United-States"))
 
def indexStringColumns(df:DataFrame, cols:Array[String]):DataFrame = {
   var newdf = df
   for(col <- cols) {
     val si = new StringIndexer().setInputCol(col).setOutputCol(col+"-num")
     val sm:StringIndexerModel = si.fit(newdf)
     newdf = sm.transform(newdf).drop(col)
     newdf = newdf.withColumnRenamed(col+"-num", col)
   }
   newdf
}

val dfnumeric = indexStringColumns(dfrawnona, Array("workclass",
 "education", "marital_status", "occupation", "relationship", "race",
 "sex", "native_country", "income"))

def oneHotEncodeColumns(df:DataFrame, cols:Array[String]):DataFrame = {
   var newdf = df
   for(c <- cols) {
     val onehotenc = new OneHotEncoder().setInputCol(c)
     onehotenc.setOutputCol(c+"-onehot").setDropLast(false)
     newdf = onehotenc.transform(newdf).drop(c)
     newdf = newdf.withColumnRenamed(c+"-onehot", c)
   }
   newdf
 }
 
val dfhot = oneHotEncodeColumns(dfnumeric, Array("workclass", "education", "marital_status", "occupation", "relationship", "race", "native_country"))


val va = new VectorAssembler().setOutputCol("features").
 setInputCols(dfhot.columns.diff(Array("income")))
 
val lpoints = va.transform(dfhot).select("features", "income").
 withColumnRenamed("income", "label")


val splits = lpoints.randomSplit(Array(0.8, 0.2))
val trainSet = splits(0).cache()
val testSet = splits(1).cache()


import org.apache.spark.ml.classification.LogisticRegression

val lr = new LogisticRegression
lr.setRegParam(0.01).setMaxIter(500).setFitIntercept(true)
val lrmodel = lr.fit(trainSet)


val validpredicts = lrmodel.transform(testSet)

import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
val bceval = new BinaryClassificationEvaluator()

bceval.evaluate(validpredicts)
