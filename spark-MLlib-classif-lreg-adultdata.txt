---- Feature extraction & Data Munging --------------

val raw = sc.textFile("adultdata/adult.data1").map(x => x.split(","))

val rdd = raw.map(x => Array(x(0),x(1).replace("?","Private"),x(2),x(3),x(4),x(5).replace("?","Prof-specialty"),
                             x(6),x(7),x(8),x(9),x(10),x(11),x(12),x(13).replace("?","United-States"),x(14)))


def oneHotEncColumns(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int):org.apache.spark.rdd.RDD[Array[Double]] = {
  val categories = rddx.map(r => r(idx)).distinct.zipWithIndex.collect.toMap
  val numCategories = categories.size
  val vetcateg = rddx.map(r => {
      val categoryIdx = categories(r(idx)).toInt
      val categoryFeatures = Array.ofDim[Double](numCategories)
      categoryFeatures(categoryIdx) = 1.0
      categoryFeatures
  })
  vetcateg
}

def mergeArray(rddx: org.apache.spark.rdd.RDD[Array[String]], idx: Int*):org.apache.spark.rdd.RDD[Array[Double]] = {
  var i = 0
  var arr1 = oneHotEncColumns(rddx,idx(i))
  for (j <- 1 until idx.size) {
    var arr2 = oneHotEncColumns(rddx,idx(j))
    var flt1 = arr1.zip(arr2).map(x => (x._1.toList ++ x._2.toList).toArray)
    arr1 = flt1
  }
  arr1
}

val concat = mergeArray(rdd,1,3,5,6,7,8,9,13)

val categories = rdd.map(r => r(14)).distinct.zipWithIndex.collect.toMap

// remove x(4) : education-num
val rdd1 = rdd.map(x => Array(categories(x(14)).toDouble,x(0).toDouble,x(2).toDouble,x(10).toDouble,x(11).toDouble,x(12).toDouble))

val vect = rdd1.zip(concat).map(x => (x._1.toList ++ x._2.toList).toArray)

import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint

val data = vect.map(r => {
   val label = r(0).toInt
   val features = r.slice(1, r.size)
   LabeledPoint(label, Vectors.dense(features))
 })
 
data.cache

val sets = data.randomSplit(Array(0.8,0.2))
val trainSet = sets(0)
val testSet = sets(1)

---- MLlib logistic regression --------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val numIterations = 100
val model = LogisticRegressionWithSGD.train(trainSet, numIterations)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 4949
validPredicts.count                            // 6554
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.6227556413221551
metrics.areaUnderROC  // 0.5003113325031133

---- MLlib SVM regression --------------

import org.apache.spark.mllib.classification.SVMWithSGD
val numIterations = 100
val model = SVMWithSGD.train(trainSet, numIterations)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 5168
validPredicts.count                            // 6554
model.getClass.getSimpleName
metrics.areaUnderPR   // 0.5957985632008835
metrics.areaUnderROC  // 0.5787969477837254

---- MLlib Naive Bayes regression --------------

import org.apache.spark.mllib.classification.NaiveBayes
val model = NaiveBayes.train(trainSet)

val validPredicts = testSet.map(x => (model.predict(x.features),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 5108
validPredicts.count                            // 6554
model.getClass.getSimpleName
metrics.areaUnderPR   //  0.4852793858100803
metrics.areaUnderROC  //  0.5958648650825371

----- Standardizing features ------------------------------

import org.apache.spark.mllib.feature.StandardScaler
val vectors = trainSet.map(lp => lp.features)
val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)
val trainScaled = trainSet.map(lp => LabeledPoint(lp.label,scaler.transform(lp.features)))

----- with MLlib logistic regression ----------------------

import org.apache.spark.mllib.classification.LogisticRegressionWithSGD
val numIterations = 100
val model = LogisticRegressionWithSGD.train(trainScaled, numIterations)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 4918
validPredicts.count                            // 6554
model.getClass.getSimpleName
metrics.areaUnderPR    // 0.4874468156388327
metrics.areaUnderROC   // 0.8140730848646596

----- with MLlib SVM regression ----------------------

import org.apache.spark.mllib.classification.SVMWithSGD
val numIterations = 100
val model = SVMWithSGD.train(trainScaled, numIterations)

val validPredicts = testSet.map(x => (model.predict(scaler.transform(x.features)),x.label))

import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics
val metrics = new BinaryClassificationMetrics(validPredicts)
validPredicts.filter(x => x._1 == x._2).count  // 4869
validPredicts.count                            // 6554
model.getClass.getSimpleName
metrics.areaUnderPR    // 0.47608954747815296
metrics.areaUnderROC   // 0.7996589185058858

----- with Naive Bayes regression ----------------------

not possible because standardization produces negative values